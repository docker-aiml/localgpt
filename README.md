# Project Brief: Integrate Local LLMs with Docker Model Runner

## üìå Project Name: `LocalGPT Assistant ‚Äì Docker Model Runner Edition`

### üë• Team Context

You're part of the "LLM Enablement" team within a developer tools org. Your squad is exploring new ways to empower AI/ML developers to build locally-hosted applications without depending on cloud APIs. You're assigned to evaluate **Docker Model Runner** and demonstrate its practical integration into an existing multi-service AI app stack.

---

## üéØ Mission Objective

> **Integrate Docker Model Runner as the local LLM backend for an existing ChatGPT-style app (FastAPI + Gradio), replacing external API calls.**

This will allow:

* Full local inference using open-source LLMs (e.g. SmolLM, TinyLlama, Gemma)

* Reproducible deployment using Docker Compose

* Offline or air-gapped development

---

## üß± Provided Project Structure

You are given the following repository:

```
localgpt/
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI backend for prompt handling
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ app.py               # Gradio frontend
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ docker-compose.yaml      # [To be created by YOU]
```

---

## üß© Your Deliverables

### üîç Phase 1: Learn and Explore Docker Model Runner

Before integrating, understand how Docker Model Runner works:

1. ‚úÖ Enable it in Docker Desktop:

   * Settings > Features in Development > Enable Docker Model Runner

   * Restart Docker Desktop

2. ‚úÖ Try out basic commands:

‚†Ä
```
docker model pull ai/smollm2
docker model run ai/smollm2 "How do you work?"
```

üëâ Observe how it pulls, loads, and responds with no external API involved.

---

### üõ†Ô∏è Phase 2: Modify FastAPI to Use Local Model

Update `app/main.py` to interact with the Model Runner's OpenAI-compatible endpoint:

```
LLM_URL = "http://model-runner.docker.internal/engines/llama.cpp/v1/chat/completions"

@app.post("/chat")
async def chat(req: Request):
    data = await req.json()
    prompt = data.get("prompt")
    payload = {
        "model": "ai/smollm2",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    }
    response = requests.post(LLM_URL, json=payload)
    return response.json()
```

‚úÖ This allows FastAPI to relay prompts to the local LLM using a standard OpenAI-style API.

---

### ‚öôÔ∏è Phase 3: Write Docker Compose Spec

Create a `docker-compose.yaml` that:

* Starts the Docker Model Runner model provider

* Boots the FastAPI and UI containers in correct sequence

* Automatically injects model metadata to services

```
version: "3"

services:
  model:
    provider:
      type: model
      options:
        model: ai/smollm2

  fastapi:
    build:
      context: ./app
    ports:
      - "8000:8000"
    depends_on:
      - model

  ui:
    build:
      context: ./ui
    ports:
      - "8501:8501"
    depends_on:
      - fastapi
```

üìå **Note**: No changes are needed in the UI ‚Äî it communicates with the FastAPI backend as before.

---

## üß™ How to Run the Stack

```
docker compose up --build
```

Visit:

* [http://localhost:8501](http://localhost:8501/) ‚Äì UI

* [http://localhost:8000/chat](http://localhost:8000/chat) ‚Äì API

---

## üß† Learning Goals

By the end of this project, you‚Äôll:

‚úÖ Understand **how Docker Model Runner manages and runs LLMs locally**
‚úÖ Replace hosted LLM APIs with **local inference endpoints**
‚úÖ Learn how to **package model providers in Docker Compose**
‚úÖ Build confidence in **open-source model deployment workflows**

---

Would you like me to generate a sample `README.md`, screenshots, or add bonus extensions like switching models or benchmarking local inference?
